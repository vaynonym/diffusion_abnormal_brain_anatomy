run:
  batch_size: 2
  input_image_downsampling_factors: [1.667, 1.75, 1.667] # [2, 2, 2]  # (160, 224, 160) ==>
  input_image_crop_roi: [96,128,96] # [80,112,80]  
  evaluation_intervall: 0.34
  dataset: "LDM100K"
  target_data: "mask"
  dataset_size: 9999

auto_encoder: 
  spatial_dims: 3
  in_channels: 64
  out_channels: 33
  latent_channels: 3
  num_channels: [64,128,128]
  num_res_blocks: 2
  norm_num_groups: 32
  attention_levels: [False, False, False]
  with_encoder_nonlocal_attn: False
  with_decoder_nonlocal_attn: False

autoencoder_training: 
  n_epochs: 40
  learning_rate: 0.000025
  autoencoder_warm_up_n_epochs: 3
  adverserial_loss: False
  adv_weight: 0.01

patch_discrim: 
  spatial_dims: 3
  num_layers_d: 3
  num_channels: 32
  in_channels: 33
  out_channels: 1

diffusion_model_unet: 
  spatial_dims: 3
  in_channels: 3
  out_channels: 3
  num_res_blocks: 2
  num_channels: [256, 512, 768]
  attention_levels: [False, True, True]
  num_head_channels: [0, 512, 768]
  norm_num_groups: 32
  upcast_attention: True
  resblock_updown: True
  with_conditioning: True
  cross_attention_dim: 2

diffusion_model_unet_training:
#  scale_loss_of_large_volume_up: True
  classifier_free_guidance: 4.0
  batch_size: 8
  n_epochs: 20
  learning_rate: 0.000005